<?xml version="1.0"?>
<doc>
    <assembly>
        <name>SharpLearning.XGBoost</name>
    </assembly>
    <members>
        <member name="T:SharpLearning.XGBoost.BoosterType">
            <summary>
            XGBoost booster types.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.BoosterType.GBTree">
            <summary>
            Gradient boosted decision trees.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.BoosterType.GBLinear">
            <summary>
            Gradient boosted linear models.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.BoosterType.DART">
            <summary>
            DART: Dropouts meet Multiple Additive Regression Trees.
            http://xgboost.readthedocs.io/en/latest/tutorials/dart.html
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.BoosterTypeExtensions">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.BoosterTypeExtensions.ToXGBoostString(SharpLearning.XGBoost.BoosterType)">
            <summary>
            Convert booster type to the xgboost parameter string.
            </summary>
            <param name="type"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.ClassificationObjective">
            <summary>
            Classification objectives.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ClassificationObjective.BinaryLogistic">
            <summary>
            logistic regression for binary classification, output probability.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ClassificationObjective.BinaryLogisticRaw">
            <summary>
            logistic regression for binary classification, output score before logistic transformation.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ClassificationObjective.GPUBinaryLogistic">
            <summary>
            GPU version of binary logistic regression evaluated on the GPU,
            note that like the GPU histogram algorithm, 
            they can only be used when the entire training session uses the same dataset.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ClassificationObjective.GPUBinaryLogisticRaw">
            <summary>
            GPU version of binary logistic regression raw evaluated on the GPU,
            note that like the GPU histogram algorithm, 
            they can only be used when the entire training session uses the same dataset.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ClassificationObjective.Softmax">
            <summary>
            Multiclass classification using the softmax objective.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ClassificationObjective.SoftProb">
            <summary>
            same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata, 
            nclass matrix.The result contains predicted probability of each data point belonging to each class.
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.ClassificationObjectiveExtensions">
            <summary>
            Classification objective extensions
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.ClassificationObjectiveExtensions.ToXGBoostString(SharpLearning.XGBoost.ClassificationObjective)">
            <summary>
            Convert classification objective to the xgboost parameter string.
            </summary>
            <param name="objective"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.Conversions">
            <summary>
            Conversions for XGBoost
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Conversions.ToFloatJaggedArray(SharpLearning.Containers.Matrices.F64Matrix)">
            <summary>
            Converts F64Matrix to float[][].
            </summary>
            <param name="matrix"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Conversions.ToFloatJaggedArray(SharpLearning.Containers.Matrices.F64Matrix,System.Int32[])">
            <summary>
            Converts F64Matrix to float[][].
            </summary>
            <param name="matrix"></param>
            <param name="rowIndices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Conversions.ToFloat(System.Double[])">
            <summary>
            Converts double array to float array.
            </summary>
            <param name="array"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Conversions.ToFloat(System.Double[],System.Int32[])">
            <summary>
            Converts double array to float array.
            </summary>
            <param name="array"></param>
            <param name="indices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Conversions.ToDouble(System.Single[])">
            <summary>
            Converts float array to double array.
            </summary>
            <param name="array"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner">
            <summary>
            Classification learner for XGBoost
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.#ctor(System.Int32,System.Double,System.Int32,System.Boolean,SharpLearning.XGBoost.ClassificationObjective,SharpLearning.XGBoost.BoosterType,SharpLearning.XGBoost.TreeMethod,SharpLearning.XGBoost.SamplerType,SharpLearning.XGBoost.NormalizeType,System.Double,System.Boolean,System.Double,System.Int32,System.Double,System.Int32,System.Int32,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Int32,System.Double)">
            <summary>
            Classification learner for XGBoost. For classification problems, 
            XGBoost requires that target values are sequential and start at 0.
            </summary>
            <param name="maximumTreeDepth">Maximum tree depth for base learners. (default is 3)</param>
            <param name="learningRate">Boosting learning rate (xgb's "eta"). 0 indicates no limit. (default is 0.1)</param>
            <param name="estimators">Number of estimators to fit. (default is 100)</param>
            <param name="silent">Whether to print messages while running boosting. (default is false)</param>
            <param name="objective">Specify the learning task and the corresponding learning objective. (default is softmax)</param>
            <param name="boosterType"> which booster to use, can be gbtree, gblinear or dart. 
            gbtree and dart use tree based model while gblinear uses linear function (default is gbtree)</param>
            <param name="treeMethod">The tree construction algorithm used in XGBoost. See reference paper: https://arxiv.org/abs/1603.02754. (default is auto)</param>
            <param name="samplerType">Type of sampling algorithm for DART. (default is uniform)</param>
            <param name="normalizeType">Type of normalization algorithm for DART. (default is tree)</param>
            <param name="dropoutRate">Dropout rate for DART (a fraction of previous trees to drop during the dropout). (default is 0.0)</param>
            <param name="oneDrop">When this is true, at least one tree is always dropped during the dropout.
            Allows Binomial-plus-one or epsilon-dropout from the original DART paper. (default is false)</param>
            <param name="skipDrop">Probability of skipping the dropout procedure during a boosting iteration. (default is 0.0)
            If a dropout is skipped, new trees are added in the same manner as gbtree.
            Note that non-zero skip_drop has higher priority than rate_drop or one_drop.</param>
            <param name="numberOfThreads">Number of parallel threads used to run xgboost. -1 means use all thread available. (default is -1)</param>
            <param name="gamma">Minimum loss reduction required to make a further partition on a leaf node of the tree. (default is 0) </param>
            <param name="minChildWeight">Minimum sum of instance weight(Hessian) needed in a child. (default is 1)</param>
            <param name="maxDeltaStep">Maximum delta step we allow each tree's weight estimation to be. (default is 0)</param>
            <param name="subSample">Subsample ratio of the training instance. (default is 1)</param>
            <param name="colSampleByTree">Subsample ratio of columns when constructing each tree. (default is 1)</param>
            <param name="colSampleByLevel">Subsample ratio of columns for each split, in each level. (default is 1)</param>
            <param name="l1Regularization">L1 regularization term on weights. Also known as RegAlpha. (default is 0)</param>
            <param name="l2Reguralization">L2 regularization term on weights. Also known as regLambda. (default is 1)</param>
            <param name="scalePosWeight">Balancing of positive and negative weights. (default is 1)</param>
            <param name="baseScore">The initial prediction score of all instances, global bias. (default is 0.5)</param>
            <param name="seed">Random number seed. (default is 0)</param>
            <param name="missing">Value in the data which needs to be present as a missing value. (default is NaN)</param>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[])">
            <summary>
            Learns an XGBoost classification model.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[],System.Int32[])">
            <summary>
            Learns an XGBoost classification model.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <param name="indices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.SharpLearning#Common#Interfaces#IIndexedLearner{System#Double}#Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[],System.Int32[])">
            <summary>
            Private explicit interface implementation for indexed learning.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <param name="indices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.SharpLearning#Common#Interfaces#IIndexedLearner{SharpLearning#Containers#ProbabilityPrediction}#Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[],System.Int32[])">
            <summary>
            Private explicit interface implementation for indexed probability learning.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <param name="indices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.SharpLearning#Common#Interfaces#ILearner{System#Double}#Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[])">
            <summary>
            Private explicit interface implementation.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.ClassificationXGBoostLearner.SharpLearning#Common#Interfaces#ILearner{SharpLearning#Containers#ProbabilityPrediction}#Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[])">
            <summary>
            Private explicit interface implementation for probability learning.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.Learners.RegressionXGBoostLearner">
            <summary>
            Regression learner for XGBoost
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.RegressionXGBoostLearner.#ctor(System.Int32,System.Double,System.Int32,System.Boolean,SharpLearning.XGBoost.RegressionObjective,SharpLearning.XGBoost.BoosterType,SharpLearning.XGBoost.TreeMethod,SharpLearning.XGBoost.SamplerType,SharpLearning.XGBoost.NormalizeType,System.Double,System.Boolean,System.Double,System.Int32,System.Double,System.Int32,System.Int32,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Double,System.Int32,System.Double)">
            <summary>
             Regression learner for XGBoost
            </summary>
            <param name="maximumTreeDepth">Maximum tree depth for base learners. (default is 3)</param>
            <param name="learningRate">Boosting learning rate (xgb's "eta"). 0 indicates no limit. (default is 0.1)</param>
            <param name="estimators">Number of estimators to fit. (default is 100)</param>
            <param name="silent">Whether to print messages while running boosting. (default is false)</param>
            <param name="objective">Specify the learning task and the corresponding learning objective. (default is LinearRegression)</param>
            <param name="boosterType"> which booster to use, can be gbtree, gblinear or dart. 
            gbtree and dart use tree based model while gblinear uses linear function (default is gbtree)</param>
            <param name="treeMethod">The tree construction algorithm used in XGBoost. See reference paper: https://arxiv.org/abs/1603.02754. (default is auto)</param>
            <param name="samplerType">Type of sampling algorithm for DART. (default is uniform)</param>
            <param name="normalizeType">Type of normalization algorithm for DART. (default is tree)</param>
            <param name="dropoutRate">Dropout rate for DART (a fraction of previous trees to drop during the dropout). (default is 0.0)</param>
            <param name="oneDrop">When this is true, at least one tree is always dropped during the dropout.
            Allows Binomial-plus-one or epsilon-dropout from the original DART paper. (default is false)</param>
            <param name="skipDrop">Probability of skipping the dropout procedure during a boosting iteration. (default is 0.0)
            If a dropout is skipped, new trees are added in the same manner as gbtree.
            Note that non-zero skip_drop has higher priority than rate_drop or one_drop.</param>
            <param name="numberOfThreads">Number of parallel threads used to run xgboost. -1 means use all thread available. (default is -1)</param>
            <param name="gamma">Minimum loss reduction required to make a further partition on a leaf node of the tree. (default is 0) </param>
            <param name="minChildWeight">Minimum sum of instance weight(Hessian) needed in a child. (default is 1)</param>
            <param name="maxDeltaStep">Maximum delta step we allow each tree's weight estimation to be. (default is 0)</param>
            <param name="subSample">Subsample ratio of the training instance. (default is 1)</param>
            <param name="colSampleByTree">Subsample ratio of columns when constructing each tree. (default is 1)</param>
            <param name="colSampleByLevel">Subsample ratio of columns for each split, in each level. (default is 1)</param>
            <param name="l1Regularization">L1 regularization term on weights. Also known as RegAlpha. (default is 0)</param>
            <param name="l2Reguralization">L2 regularization term on weights. Also known as regLambda. (default is 1)</param>
            <param name="scalePosWeight">Balancing of positive and negative weights. (default is 1)</param>
            <param name="baseScore">The initial prediction score of all instances, global bias. (default is 0.5)</param>
            <param name="seed">Random number seed. (default is 0)</param>
            <param name="missing">Value in the data which needs to be present as a missing value. (default is NaN)</param>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.RegressionXGBoostLearner.Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[])">
            <summary>
            Learns an XGBoost regression model.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.RegressionXGBoostLearner.Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[],System.Int32[])">
            <summary>
            Learns an XGBoost regression model.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <param name="indices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.RegressionXGBoostLearner.SharpLearning#Common#Interfaces#IIndexedLearner{System#Double}#Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[],System.Int32[])">
            <summary>
            Private explicit interface implementation for indexed learning.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <param name="indices"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Learners.RegressionXGBoostLearner.SharpLearning#Common#Interfaces#ILearner{System#Double}#Learn(SharpLearning.Containers.Matrices.F64Matrix,System.Double[])">
            <summary>
            Private explicit interface implementation for indexed learning.
            </summary>
            <param name="observations"></param>
            <param name="targets"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.Models.ClassificationXGBoostModel">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.#ctor(XGBoost.lib.Booster)">
            <summary>
            
            </summary>
            <param name="model"></param>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.Predict(System.Double[])">
            <summary>
            
            </summary>
            <param name="observation"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.Predict(SharpLearning.Containers.Matrices.F64Matrix)">
            <summary>
            
            </summary>
            <param name="observations"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.PredictProbability(System.Double[])">
            <summary>
            
            </summary>
            <param name="observation"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.PredictProbability(SharpLearning.Containers.Matrices.F64Matrix)">
            <summary>
            
            </summary>
            <param name="observations"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.GetRawVariableImportance">
            <summary>
            Raw variable importance is not supported by XGBoost models.
            </summary>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.GetVariableImportance(System.Collections.Generic.Dictionary{System.String,System.Int32})">
            <summary>
            Returns the rescaled (0-100) and sorted variable importance scores with corresponding name
            </summary>
            <param name="featureNameToIndex"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.Load(System.String)">
            <summary>
            Loads a ClassificationXGBoostModel.
            </summary>
            <param name="modelFilePath"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.Save(System.String)">
            <summary>
            Saves the ClassificationXGBoostModel.
            </summary>
            <param name="modelFilePath"></param>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.ClassificationXGBoostModel.Dispose">
            <summary>
            
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.Models.FeatureImportanceParser">
            <summary>
            Parser for XGBoost feature importance.
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.FeatureImportanceParser.ParseFromTreeDump(System.String[],System.Int32)">
            <summary>
            Parse array of feature importance values from text dump of XGBoost trees.
            </summary>
            <param name="textTrees"></param>
            <param name="numberOfFeatures"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.Models.RegressionXGBoostModel">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.#ctor(XGBoost.lib.Booster)">
            <summary>
            
            </summary>
            <param name="model"></param>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.Predict(System.Double[])">
            <summary>
            
            </summary>
            <param name="observation"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.Predict(SharpLearning.Containers.Matrices.F64Matrix)">
            <summary>
            
            </summary>
            <param name="observations"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.GetRawVariableImportance">
            <summary>
            Raw variable importance is not supported by XGBoost models.
            </summary>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.GetVariableImportance(System.Collections.Generic.Dictionary{System.String,System.Int32})">
            <summary>
            Returns the rescaled (0-100) and sorted variable importance scores with corresponding name
            </summary>
            <param name="featureNameToIndex"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.Load(System.String)">
            <summary>
            Loads a RegressionXGBoostModel.
            </summary>
            <param name="modelFilePath"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.Save(System.String)">
            <summary>
            Saves the RegressionXGBoostModel.
            </summary>
            <param name="modelFilePath"></param>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.RegressionXGBoostModel.Dispose">
            <summary>
            
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.Models.XGBoostTreeConverter">
            <summary>
            Conversion between xgboost trees text format and SharpLearning.GradientBoost GBMTrees.
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.FromXGBoostTextTreesToGBMTrees(System.String[])">
            <summary>
            Parse array of feature importance values from text dump of XGBoost trees.
            </summary>
            <param name="textTrees"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ConvertXGBoostTextTreeToGBMTree(System.String)">
            <summary>
            Converts a single XGBoost tree in text format to a GBMTree.
            </summary>
            <param name="textTree"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.IsLeaf(System.String)">
            <summary>
            Checks if the current line contains a leaf node.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ParseLeafValue(System.String)">
            <summary>
            Parses the leaf value from a line.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ParseFeatureIndex(System.String)">
            <summary>
            Parses the feature index from a line.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ParseSplitValue(System.String)">
            <summary>
            Parses the split value from a line.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ParseYesIndex(System.String)">
            <summary>
            Parses the Yes node index from a line.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ParseNoIndex(System.String)">
            <summary>
            Parses the No index from a line.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="M:SharpLearning.XGBoost.Models.XGBoostTreeConverter.ParseNodeIndex(System.String)">
            <summary>
            Parses the node index from a line.
            </summary>
            <param name="line"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.NormalizeType">
            <summary>
            Type of normalization algorithm for DART
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.NormalizeType.Tree">
            <summary>
            New trees have the same weight of each of dropped trees.
            Weight of new trees are 1 / (k + learning_rate).
            Dropped trees are scaled by a factor of k / (k + learning_rate).
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.NormalizeType.Forest">
            <summary>
            New trees have the same weight of sum of dropped trees(forest).
            Weight of new trees are 1 / (1 + learning_rate).
            Dropped trees are scaled by a factor of 1 / (1 + learning_rate)
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.NormalizeTypeExtensions">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.NormalizeTypeExtensions.ToXGBoostString(SharpLearning.XGBoost.NormalizeType)">
            <summary>
            Convert normalize type to the xgboost parameter string.
            </summary>
            <param name="type"></param>
            <returns></returns>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.MaxDepth">
            <summary>
            Maximum tree depth for base learners
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.LearningRate">
            <summary>
            Boosting learning rate (xgb's "eta")
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Estimators">
            <summary>
            Number of boosted trees to fit
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Silent">
            <summary>
            Whether to print messages while running boosting
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.objective">
            <summary>
            Specify the learning task and the corresponding learning objective or
            a custom objective function to be used(see note below)
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Threads">
            <summary>
            Number of parallel threads used to run xgboost
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Gamma">
            <summary>
            Minimum loss reduction required to make a further partition on a leaf node of the tree
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.MinChildWeight">
            <summary>
            Minimum sum of instance weight(hessian) needed in a child
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.MaxDeltaStep">
            <summary>
            Maximum delta step we allow each tree's weight estimation to be
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.SubSample">
            <summary>
            Subsample ratio of the training instance
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.ColSampleByTree">
            <summary>
            Subsample ratio of columns when constructing each tree TODO prevent error for bigger range of vals
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.ColSampleByLevel">
            <summary>
            Subsample ratio of columns for each split, in each level TODO prevent error for bigger range of vals
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.RegAlpha">
            <summary>
            L1 regularization term on weights
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.RegLambda">
            <summary>
            L2 regularization term on weights
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.ScalePosWeight">
            <summary>
            Balancing of positive and negative weights
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.BaseScore">
            <summary>
            The initial prediction score of all instances, global bias
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Seed">
            <summary>
            Random number seed
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Missing">
            <summary>
            Value in the data which needs to be present as a missing value
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.ExistingBooster">
            <summary>
            Existing booster
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.Booster">
            <summary>
            which booster to use, can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.TreeMethod">
            <summary>
            The tree construction algorithm used in XGBoost. See reference paper: https://arxiv.org/abs/1603.02754
            Distributed and external memory version only support approximate algorithm.
            Choices: {'auto', 'exact', 'approx', 'hist', 'gpu_exact', 'gpu_hist'}.
            'auto': Use heuristic to choose faster one.
            - For small to medium dataset, exact greedy will be used.
            - For very large-dataset, approximate algorithm will be chosen.
            - Because old behavior is always use exact greedy in single machine, 
            user will get a message when approximate algorithm is chosen to notify this choice.
            
            'exact': Exact greedy algorithm.
            'approx': Approximate greedy algorithm using sketching and histogram.
            'hist': Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching.
            'gpu_exact': GPU implementation of exact algorithm.
            'gpu_hist': GPU implementation of hist algorithm.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.NumberOfClasses">
            <summary>
            The number of classes in a classification problem.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.SampleType">
            <summary>
            type of sampling algorithm for DART.
            - "uniform": dropped trees are selected uniformly.
            - "weighted": dropped trees are selected in proportion to weight.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.NormalizeType">
            <summary>
            type of normalization algorithm for DART.
            -"tree": new trees have the same weight of each of dropped trees.
              - weight of new trees are 1 / (k + learning_rate)
                dropped trees are scaled by a factor of k / (k + learning_rate)
            - "forest": new trees have the same weight of sum of dropped trees(forest).
              - weight of new trees are 1 / (1 + learning_rate)
                dropped trees are scaled by a factor of 1 / (1 + learning_rate)
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.RateDrop">
            <summary>
            Dropout rate for DART. (a fraction of previous trees to drop during the dropout).
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.OneDrop">
            <summary>
            One drop for DART. 
            When this flag is enabled, 
            at least one tree is always dropped during the dropout 
            (allows Binomial-plus-one or epsilon-dropout from the original DART paper.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.ParameterNames.SkipDrop">
            <summary>
            Skip_drop for DART. Probability of skipping the dropout procedure during a boosting iteration.
             - If a dropout is skipped, new trees are added in the same manner as gbtree.
             - Note that non-zero skip_drop has higher priority than rate_drop or one_drop.
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.RegressionObjective">
            <summary>
            Regression objectives.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.LinearRegression">
            <summary>
            linear regression.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.LogisticRegression">
            <summary>
            logistic regression.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.GPULinear">
            <summary>
            GPU version of linear regression evaluated on the GPU,
            note that like the GPU histogram algorithm, 
            they can only be used when the entire training session uses the same dataset.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.GPULogistic">
            <summary>
            GPU version of logistic regression evaluated on the GPU,
            note that like the GPU histogram algorithm, 
            they can only be used when the entire training session uses the same dataset.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.CountPoisson">
            <summary>
            Poisson regression for count data, output mean of Poisson distribution,
            max_delta_step is set to 0.7 by default in Poisson regression (used to safeguard optimization).
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.SurvivalCox">
            <summary>
            Cox regression for right censored survival time data (negative values are considered right censored). 
            Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) 
            in the proportional hazard function h(t) = h0(t) * HR).
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.RankPairwise">
            <summary>
            ranking task by minimizing the pairwise loss.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.GammaRegression">
            <summary>
            gamma regression with log-link.Output is a mean of gamma distribution.
            It might be useful, e.g., for modeling insurance claims severity, 
            or for any outcome that might be gamma-distributed
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.RegressionObjective.TweedieRegression">
            <summary>
            Tweedie regression with log-link.It might be useful, e.g., 
            for modeling total loss in insurance, 
            or for any outcome that might be Tweedie-distributed.
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.RegressionObjectiveExtensions">
            <summary>
            Regression objective extensions
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.RegressionObjectiveExtensions.ToXGBoostString(SharpLearning.XGBoost.RegressionObjective)">
            <summary>
            Convert regression objective to the xgboost parameter string.
            </summary>
            <param name="objective"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.SamplerType">
            <summary>
            Sampler type for DART
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.SamplerType.Uniform">
            <summary>
            Dropped trees are selected uniformly.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.SamplerType.Weighted">
            <summary>
            Dropped trees are selected in proportion to weight.
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.SamplerTypeExtensions">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.SamplerTypeExtensions.ToXGBoostString(SharpLearning.XGBoost.SamplerType)">
            <summary>
            Convert sampler type to the xgboost parameter string.
            </summary>
            <param name="type"></param>
            <returns></returns>
        </member>
        <member name="T:SharpLearning.XGBoost.TreeMethod">
            <summary>
            XGBoost tree methods.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.TreeMethod.Auto">
            <summary>
            Auto: Use heuristic to choose faster one.
            - For small to medium dataset, exact greedy will be used.
            - For very large-dataset, approximate algorithm will be chosen.
            - Because old behavior is always use exact greedy in single machine, 
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.TreeMethod.Exact">
            <summary>
            Exact greedy algorithm
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.TreeMethod.Approx">
            <summary>
            Approximate greedy algorithm using sketching and histogram.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.TreeMethod.Hist">
            <summary>
            Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.TreeMethod.GPUExact">
            <summary>
            GPU implementation of exact algorithm.
            </summary>
        </member>
        <member name="F:SharpLearning.XGBoost.TreeMethod.GPUHist">
            <summary>
            GPU implementation of hist algorithm.
            </summary>
        </member>
        <member name="T:SharpLearning.XGBoost.TreeMethodExtensions">
            <summary>
            
            </summary>
        </member>
        <member name="M:SharpLearning.XGBoost.TreeMethodExtensions.ToXGBoostString(SharpLearning.XGBoost.TreeMethod)">
            <summary>
            Convert regression objective to the xgboost parameter string.
            </summary>
            <param name="type"></param>
            <returns></returns>
        </member>
    </members>
</doc>
